{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peX_6uD-ifG1"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"https://github.com/HSG-AIML-Teaching/GSERM2022-Lab/blob/main/exam/banner.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0cP5Z789_rr"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/HSG-AIML-Teaching/GSERM2022-Lab/blob/main/exam/hsg_logo.png?raw=1\">\n",
    "\n",
    "##  Exam - Fashion MNIST Convolutional Neural Networks (CNNs)\n",
    "\n",
    "GSERM'22 course \"Deep Learning: Fundamentals and Applications\", University of St. Gallen\n",
    "\n",
    "Kayla Kahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rno8GqfC9_rz"
   },
   "source": [
    "This week we have learned how to enhance vanilla Artificial Neural Networks (ANNs) using `PyTorch` to classify even more complex images. For this purpose, we used a special type of deep neural network referred to **Convolutional Neural Networks (CNNs)**. In our exam exercise, we aim to leverage that knowledge by applying it to the known Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93JK2DH9_r0"
   },
   "source": [
    "As always, pls. don't hesitate to post your potential questions on CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email: aiml-teaching.ics@unisg.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW6dySzs9_r1"
   },
   "source": [
    "## 1. Assignment Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uzc9Xr69_r1"
   },
   "source": [
    "As discussed in our last session, these are the tasks for the exam exercise:\n",
    "\n",
    "> 1. Load the Fashion **MNIST dataset**. \n",
    "> 2. Implement a **CNN** architecture able to work with this data.\n",
    "> 3. **Train** the model, evaluate its **performance** and visualize the results.\n",
    "> 4. Try to **improve** the performance of the initial model.\n",
    "> 5. **Document** the results in the form of PowerPoint slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU1bsPMajc7o"
   },
   "source": [
    "But before we do so let's start (as always) with a motivational video by NVIDIA. Happy coding! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPzLlT4pjc7p"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# Official Intro | GTC 2020 | I AM AI\"\n",
    "# YouTubeVideo('e2_hsjpTi4w', width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPRKkkig9_r2"
   },
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mZL4i6W9_r2"
   },
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the `PyTorch`, `Numpy`, `Sklearn`, `Matplotlib`, `Seaborn` and a few utility libraries throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9cwWtab9_r2"
   },
   "outputs": [],
   "source": [
    "# import standard python libraries\n",
    "import os, urllib, io\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrB_51t89_r3"
   },
   "source": [
    "Import Python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH6LhB_q9_r3"
   },
   "outputs": [],
   "source": [
    "# import the PyTorch deep learning library\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfgYux7K9_r3"
   },
   "source": [
    "Import the sklearn classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFptYrnr9_r4"
   },
   "outputs": [],
   "source": [
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJJ5kfaf9_r4"
   },
   "source": [
    "Import Python plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usAgsocK9_r4"
   },
   "outputs": [],
   "source": [
    "# import matplotlib, seaborn, and PIL data visualization libary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZft6q1B9_r5"
   },
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXnX3zt_9_r5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn2cf5SqJ2m9"
   },
   "source": [
    "Import Google's GDrive connector and mount your GDrive directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2rj2ThhJ3sA"
   },
   "outputs": [],
   "source": [
    "# import the Google Colab GDrive connector\n",
    "from google.colab import drive\n",
    "\n",
    "# mount GDrive inside the Colab notebook\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-58e-iazJ8Aq"
   },
   "source": [
    "Create a structure of Colab Notebook sub-directories inside of GDrive to store (1) the data as well as (2) the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtB6DCWjJ-gD"
   },
   "outputs": [],
   "source": [
    "# create Colab Notebooks directory\n",
    "notebook_directory = '/content/drive/MyDrive/Colab Notebooks'\n",
    "if not os.path.exists(notebook_directory): os.makedirs(notebook_directory)\n",
    "\n",
    " # create data sub-directory inside the Colab Notebooks directory\n",
    "data_directory = '/content/drive/MyDrive/Colab Notebooks/data'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    " # create models sub-directory inside the Colab Notebooks directory\n",
    "models_directory = '/content/drive/MyDrive/Colab Notebooks/models'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcYgp4Gl9_r6"
   },
   "source": [
    "Set a random `seed` value to obtain reproducable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdbqEjHb9_r7"
   },
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpKQNDr09_r7"
   },
   "source": [
    "Google Colab provides the use of free GPUs for running notebooks. However, if you just execute this notebook as is, it will use your device's CPU. To run the lab on a GPU, got to `Runtime` > `Change runtime type` and set the Runtime type to `GPU` in the drop-down. Running this lab on a CPU is fine, but you will find that GPU computing is faster. *CUDA* indicates that the lab is being run on GPU.\n",
    "\n",
    "Enable GPU computing by setting the `device` flag and init a `CUDA` seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAFg7INc9_r7"
   },
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-7Ve4-_9_r7"
   },
   "source": [
    "Let's determine if we have access to a GPU provided by e.g. Google's COLab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCpTB9x59_r8"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mZNLbFQifHF"
   },
   "source": [
    "## 3. Exam Exercise: Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUd6ScWXmkel"
   },
   "source": [
    "The **Fashion-MNIST database** is a large database of Zalando articles that is commonly used for training various image processing systems. The database is widely used for training and testing in the field of machine learning. Source: https://www.kaggle.com/c/insar-fashion-mnist-challenge\n",
    "\n",
    "The Fashion-MNIST database has 70,000 total images and is pre-divided into 60,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH1CSkRV9_r8"
   },
   "source": [
    "### 3.1 Fashion-MNIST Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCCLgvKhmQ4q"
   },
   "source": [
    "Set a directory to store the training data when it is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayTZlAdwm5au"
   },
   "outputs": [],
   "source": [
    "# define train path\n",
    "# 'data_directory' is defined in a chunk above\n",
    "train_path = data_directory + '/train_fmnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9PfNoxknjVB"
   },
   "source": [
    "Next we download the training data. We first define a transformation to download the images as tensor format, or 28x28 matrices of pixels of grayscale values. This is what the computer understands rather than what the human eye sees/understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2r9v9W8n4sd"
   },
   "outputs": [],
   "source": [
    "# define transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# download and transform the training images\n",
    "# use the train path we defined above, train = TRUE says to use train not evaluation set,\n",
    "# use the transformation we just defined\n",
    "fashion_mnist_train_data = torchvision.datasets.FashionMNIST(root=train_path, train=True, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qny15do3oosn"
   },
   "source": [
    "To make sure it downloaded correctly, we can assess the length of the data we just downloaded. It should be 60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CORV4tNooDs"
   },
   "outputs": [],
   "source": [
    "len(fashion_mnist_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuJ4togSqBbd"
   },
   "source": [
    "We can also look at some of the images. The transformation we used said to download as a tensor, so that is what we should get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s324R_q7qUMh"
   },
   "outputs": [],
   "source": [
    "# select an image id\n",
    "image_id = 29000\n",
    "\n",
    "# retrieve the image that has this id\n",
    "fashion_mnist_train_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLUOFSExq8Kp"
   },
   "source": [
    "We see with the image information above that it presents as a tensor. The very last number (7 for this chosen id) is the image label. We can also separate the image from the label. The labels are numbers but we can map this to the fashion item (shown on Zalando github page) so that the labels have meaning for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA6Lw2qkrNCb"
   },
   "outputs": [],
   "source": [
    "# separate image from label\n",
    "fashion_mnist_train_image, fashion_mnist_train_label = fashion_mnist_train_data[image_id]\n",
    "\n",
    "# see the label for the image we looked at\n",
    "print(fashion_mnist_train_label)\n",
    "\n",
    "# map number labels to fashion items\n",
    "fashion_classes = {0: 'T-shirt/top',\n",
    "                    1: 'Trouser',\n",
    "                    2: 'Pullover',\n",
    "                    3: 'Dress',\n",
    "                    4: 'Coat',\n",
    "                    5: 'Sandal',\n",
    "                    6: 'Shirt',\n",
    "                    7: 'Sneaker',\n",
    "                    8: 'Bag',\n",
    "                    9: 'Ankle boot'}\n",
    "\n",
    "# see the fashion item that the label represents\n",
    "fashion_classes[fashion_mnist_train_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y97apD4qx5Tf"
   },
   "source": [
    "So we see that our chosen image id is label 7 which is sneaker. \n",
    "\n",
    "We can also turn the image from a tensor to an image that the human eye can make sense of so that we can look at whether this is really a sneaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSDBhXO5z04_"
   },
   "outputs": [],
   "source": [
    "# tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title\n",
    "plt.title('Example: {}, Label: {}'.format(str(image_id), fashion_classes[fashion_mnist_train_label]))\n",
    "\n",
    "# plot the fashion image\n",
    "plt.imshow(trans(fashion_mnist_train_image), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC_ya3dq3iYY"
   },
   "source": [
    "We also need the evaluation data. We define the directory and additionally define a transformation to tensors just as we did for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuMbMSJ13loS"
   },
   "outputs": [],
   "source": [
    "# define the directory in which to store the evaluation data\n",
    "eval_path = data_directory + '/eval_fmnist'\n",
    "\n",
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# download and transform training images\n",
    "# train set to False this time in order to get the evaluation set instead of the train set\n",
    "fashion_mnist_eval_data = torchvision.datasets.FashionMNIST(root=eval_path, train=False, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZih7Hte4kAk"
   },
   "source": [
    "Check the length of the evaluation data as a verification. It should be 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMn8Y0Yv4iAh"
   },
   "outputs": [],
   "source": [
    "len(fashion_mnist_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9Xivz3j9_sD"
   },
   "source": [
    "### 3.2 Convolutional Neural Network (CNN) Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nswYOXvk9_r0"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/HSG-AIML/LabAI-Coding/blob/main/resources/lab_05/classification.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgqmaa129_sZ"
   },
   "source": [
    "Please note this image of a CNN was defined for the CIFAR-10 dataset. Your Fashion-MNIST images have a different size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn2Wu-JaoXyJ"
   },
   "source": [
    "####**1. Implement and train your \"baseline\" CNN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30lRuG-k78y3"
   },
   "source": [
    "First we define the architecture of the baseline CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIXxqBfdLZ9A"
   },
   "outputs": [],
   "source": [
    "# implement the network architecture\n",
    "# (nn.Module gets the functionalities from the pytorch module)\n",
    "class FashionMnistCNN(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call super class constructor\n",
    "        super(FashionMnistCNN, self).__init__()\n",
    "\n",
    "        # specify convolution layer 1. (1 input channel w/ grayscale. output = features to extract)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # specify convolution layer 2. (input channels = output from before)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
    "        # define max-pooling layer 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # classification part\n",
    "        # specify first fully connected layer\n",
    "        self.linear1 = nn.Linear(64*4*4, 600, bias=True) # linearity\n",
    "        self.relu1 = nn.ReLU(inplace=True) # non-linearity\n",
    "        # fully connected layer 2\n",
    "        self.linear2 = nn.Linear(600, 120, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # fully connected layer 3 (output is 10 for 10 fashion mnist classes)\n",
    "        self.linear3 = nn.Linear(120, 10)\n",
    "        # feed output of 3rd fc through softmax\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    # where it is glued together\n",
    "    # define the forward pass\n",
    "    def forward(self, images):\n",
    "        \n",
    "        # high level feature learning via convolutional layers\n",
    "        # define conv layer 1 forward pass, max pooling, relu sneaks in a nonlinearity to help with learning\n",
    "        x = self.pool1(self.relu1(self.conv1(images)))\n",
    "        # define conv layer 2 forward pass\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "\n",
    "        # feature flattening\n",
    "        # reshape image pixels\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # combination of feature learning via non-linear layers\n",
    "        # define fully connected layer 1 forward pass\n",
    "        x = self.relu1(self.linear1(x))\n",
    "        # define fc layer 2 forward pass\n",
    "        x = self.relu2(self.linear2(x))\n",
    "        # define layer 3 forward pass\n",
    "        x = self.logsoftmax(self.linear3(x))\n",
    "\n",
    "        # return forward pass result\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVB2K01Lh7X-"
   },
   "source": [
    "Initialize model to be trained and push it to the device that we enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ktDX1XgiH5O"
   },
   "outputs": [],
   "source": [
    "# instantiate model \n",
    "model = FashionMnistCNN()\n",
    "# push to device\n",
    "model = model.to(device)\n",
    "# if GPU is enabled we can double check that the model was pushed there\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZB_88Wzijtn"
   },
   "source": [
    "Now that the model is initialized we can take a look at the network architecture to review the model structure. We can also look at the number of parameters that we will be training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcyQhkz1i7dG"
   },
   "outputs": [],
   "source": [
    "# look at the architecture\n",
    "print('[LOG] FashionMnistCNN architecture:\\n\\n{}\\n'.format(model))\n",
    "\n",
    "# look at the parameters\n",
    "# first initialize the number of paramters\n",
    "num_params = 0\n",
    "# iterate over the distinct parameters\n",
    "for param in model.parameters():\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "#print number of parameters\n",
    "print('[LOG] Number of FashionMnistCNN model parameters to be trained: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbCqdH5pq_3K"
   },
   "source": [
    "We use negative log-likelihood loss which penalizes models that have a high error between the predicted class and true class. The loss function should be pushed to the same computing device that the model is on. We also need to define the learning rate and optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu4M3e6arStr"
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "nll_loss = nn.NLLLoss()\n",
    "#push initialized loss function to device\n",
    "nll_loss = nll_loss.to(device)\n",
    "\n",
    "# define learning rate\n",
    "learning_rate = .001\n",
    "# define optimization strategy (stochastic gradient descent)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SUKQib-wTRV"
   },
   "source": [
    "For training the model, we will use a mini-batch size of 128 images per batch and will train for 20 epochs. 20 epochs says that we will show the 60,000 training images to the network 20 times as part of the training process. The mini-batch size says that the dataset being fed to the network 20 times will be done in chunks of 128 images. \n",
    "\n",
    "We also initialize a dataloader to feed the image tensors to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0gbfZsg34fV"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "mini_batch_size = 128\n",
    "\n",
    "# dataloader\n",
    "fashion_mnist_train_dataloader = torch.utils.data.DataLoader(fashion_mnist_train_data, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd3b3eMDHwFG"
   },
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPTxwlMbHxG_"
   },
   "outputs": [],
   "source": [
    "# initialize collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set model in training mode\n",
    "model.train()\n",
    "\n",
    "# training the model\n",
    "# iterate over the 20 epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # initialize collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "    # iterate over all mini-batches\n",
    "    for i, (images, labels) in enumerate(fashion_mnist_train_dataloader):\n",
    "        # push mini-batch data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = model(images)\n",
    "        # reset graph gradients\n",
    "        model.zero_grad()\n",
    "        #determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "\n",
    "        # backward pass -- take the loss\n",
    "        loss.backward()\n",
    "        # calculate new parameters based on the loss\n",
    "        optimizer.step()\n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "    \n",
    "    # determine mean mini-batch loss for epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "\n",
    "    # set filename of model -- generate new name for each epoch\n",
    "    model_name = 'fashion_mnist_model_epoch_{}.pth'.format(str(epoch))\n",
    "    # save current model to gdrive models directory\n",
    "    torch.save(model.state_dict(), os.path.join(models_directory, model_name))\n",
    "\n",
    "    # append mean mini-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ijijrYQ8eL"
   },
   "source": [
    "Now that the model has been trained, we can plot the training loss per epoch in order to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1c9QX4UQ70V"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid(linestyle='dotted') # add grid\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss')\n",
    "\n",
    "# add axis labels and title\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw_HXz2SCUDS"
   },
   "source": [
    "We can see in the above plot of the loss that the model is definitely learning with each epoch, but the loss is still pretty high. We also can see that 20 epochs was not enough time to converge - the model looks like it was still learning (i.e. the loss would still have dropped quite a bit if given more epochs).\n",
    "\n",
    "Now we are ready to evaluate the model with the images it has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CegsA4deVIpg"
   },
   "source": [
    "####Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnWbJSFLVftN"
   },
   "source": [
    "First we will load the best performing model from our model snapshots from training. In this case, it was the last model snapshot - the 19th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tYMswTyYkzC"
   },
   "outputs": [],
   "source": [
    "# restore pre-trained model snapshot\n",
    "best_model_name = '/content/drive/MyDrive/Colab Notebooks/models/fashion_mnist_model_epoch_19.pth'\n",
    "\n",
    "# init pre-trained model class\n",
    "best_model = FashionMnistCNN()\n",
    "\n",
    "# load epoch 19 (best performing epoch) of trained model and put it on cpu (as opposed to gpu)\n",
    "best_model.load_state_dict(torch.load(best_model_name, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxtDaoovICCH"
   },
   "source": [
    "We can make sure the model loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glgHmEocIE4c"
   },
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rb0zKqwJABM"
   },
   "source": [
    "We can use the PyTorch dataloader for the evaluation images, which are the images that the model has not seen yet (the images the model was not trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP2Sm4_cJJBr"
   },
   "outputs": [],
   "source": [
    "fashion_mnist_eval_dataloader = torch.utils.data.DataLoader(fashion_mnist_eval_data, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K_QMu5QJc0Y"
   },
   "source": [
    "Evaluate the model with the mini-batch approach and compute the mean negative log-likelihood loss of all of the mini-batches that are processed in an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNMnk60hJ4Pa"
   },
   "outputs": [],
   "source": [
    "# initialize collection of mini-batch losses\n",
    "eval_mini_batch_losses = []\n",
    "\n",
    "# iterate over all mini-batches\n",
    "for i, (images, labels) in enumerate(fashion_mnist_eval_dataloader):\n",
    "    \n",
    "    # forward pass\n",
    "    output = best_model(images)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = nll_loss(output, labels)\n",
    "\n",
    "    # collect mini-batch reconstruction loss\n",
    "    eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "# determine mean mini-batch loss for epoch\n",
    "eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "# print epoch loss\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQgeDEnsLhf4"
   },
   "source": [
    "We can see that the evaluation loss is 0.8203 which is not far off from the training loss at the last epoch.\n",
    "\n",
    "We can now get the predictions for all the evaluation data observations in order to calculate the overall classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx8F6_tKLng9"
   },
   "outputs": [],
   "source": [
    "# preds\n",
    "predictions = torch.argmax(best_model(iter(fashion_mnist_eval_dataloader).next()[0]), dim=1)\n",
    "\n",
    "# accuracy\n",
    "metrics.accuracy_score(fashion_mnist_eval_data.targets, predictions.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WUL_8HzrXzy"
   },
   "source": [
    "At 0.6918, the classification accuracy is pretty low, but still better than guessing at random. With 10 categories, guessing at random would have a classification accuracy of about 0.1. \n",
    "\n",
    "We can also see the confusion matrix to see the (mis)classification of each category in reference to all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCYNZg27rkFs"
   },
   "outputs": [],
   "source": [
    "# define classification matrix of the predicted and target classes\n",
    "mat = confusion_matrix(fashion_mnist_eval_data.targets, predictions.detach())\n",
    "# initialize the plot and define size\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# plot the confusion matrix as a heat map\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=fashion_classes, yticklabels=fashion_classes)\n",
    "plt.tick_params(axis='both', which='major', labelsize=8, labelbottom=False, bottom=False, top=False, left=False, labeltop=True)\n",
    "\n",
    "# plot title and axis labels\n",
    "plt.title('Fashion MNIST Classification Matrix\\nAccuracy = 0.6918; Loss=0.8203')\n",
    "plt.xlabel('[true class]')\n",
    "plt.ylabel('[predicted class]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNBnGfwU9_sa"
   },
   "source": [
    "###**3.3 Improved CNN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT0RaUzt_jjL"
   },
   "source": [
    "#### Implement and train your \"improved\" CNN\n",
    "(\"improved\" simply refers to a better classification accuracy than your baseline model)\n",
    "\n",
    "This model architecture is similar to the baseline model but will include dropout regularization within the architecture with a dropout probability of 50% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbwYKivl_YjU"
   },
   "outputs": [],
   "source": [
    "# implement the network architecture\n",
    "# (nn.Module gets the functionalities from the pytorch module)\n",
    "class FashionMnistCNN(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call super class constructor\n",
    "        super(FashionMnistCNN, self).__init__()\n",
    "\n",
    "        # specify convolution layer 1. (1 input channel w/ grayscale. output = features to extract)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # specify convolution layer 2. (input channels = output from before)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
    "        # define max-pooling layer 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # classification part\n",
    "        # specify first fully connected layer\n",
    "        self.linear1 = nn.Linear(64*4*4, 600, bias=True) # linearity\n",
    "        self.relu1 = nn.ReLU(inplace=True) # non-linearity\n",
    "        # fully connected layer 2\n",
    "        self.linear2 = nn.Linear(600, 120, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # fully connected layer 3 (output is 10 for 10 fashion mnist classes)\n",
    "        self.linear3 = nn.Linear(120, 10)\n",
    "        # feed output of 3rd fc through softmax\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.5, inplace=True)\n",
    "    \n",
    "    # where it is glued together\n",
    "    # define the forward pass\n",
    "    def forward(self, images):\n",
    "        \n",
    "        # high level feature learning via convolutional layers\n",
    "        # define conv layer 1 forward pass, max pooling, relu sneaks in a nonlinearity to help with learning\n",
    "        x = self.pool1(self.relu1(self.conv1(images)))\n",
    "        # define conv layer 2 forward pass\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "\n",
    "        # feature flattening\n",
    "        # reshape image pixels\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # combination of feature learning via non-linear layers\n",
    "        # define fully connected layer 1 forward pass\n",
    "        x = self.relu1(self.dropout(self.linear1(x)))\n",
    "        # define fc layer 2 forward pass\n",
    "        x = self.relu2(self.dropout(self.linear2(x)))\n",
    "        # define layer 3 forward pass\n",
    "        x = self.logsoftmax(self.linear3(x))\n",
    "\n",
    "        # return forward pass result\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_yOC9PZGVax"
   },
   "source": [
    "Instantiate model and push to the computing device that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o1SZvAF_YFu"
   },
   "outputs": [],
   "source": [
    "# instantiate model \n",
    "model = FashionMnistCNN()\n",
    "# push to device\n",
    "model = model.to(device)\n",
    "# if GPU is enabled  we can double check that the model was pushed there\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS9Pt-q-ERn6"
   },
   "source": [
    "Now we can view the architectural structure and the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgRZ1VtUGhxm"
   },
   "outputs": [],
   "source": [
    "# look at the architecture\n",
    "print('[LOG] FashionMnistCNN architecture:\\n\\n{}\\n'.format(model))\n",
    "\n",
    "# look at the parameters\n",
    "# first initialize the number of paramters\n",
    "num_params = 0\n",
    "# iterate over the distinct parameters\n",
    "for param in model.parameters():\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "#print number of parameters\n",
    "print('[LOG] Number of FashionMnistCNN model parameters to be trained: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1BAnJPaH1sb"
   },
   "source": [
    "We will use negative log likelihood loss again. This time we will use the Adam optimizer which adjusts the learning rate, but we do set the initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di13_037Gs69"
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "nll_loss = nn.NLLLoss()\n",
    "#push initialized loss function to device\n",
    "nll_loss = nll_loss.to(device)\n",
    "\n",
    "# set the initial learning rate\n",
    "learning_rate = .0001\n",
    "# define optimization strategy (Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiFaUMZnIsjp"
   },
   "source": [
    "We will also use 40 epochs. From the base model, we saw that on 20 epochs, it was starting to converge but still needed some more epochs to be able to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQFIY1UYGy15"
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "mini_batch_size = 128\n",
    "\n",
    "# dataloader\n",
    "fashion_mnist_train_dataloader = torch.utils.data.DataLoader(fashion_mnist_train_data, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAithoPcldae"
   },
   "source": [
    "Now we can train the (theoretically) better model. Note that the way this is named/stored, it will overwrite the model we trained above. That is ok for now since the model above trained so quickly - if I needed to go back and redo anything I could make a new folder and rename the other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vzx8-ydHHLO7"
   },
   "outputs": [],
   "source": [
    "# initialize collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set model in training mode\n",
    "model.train()\n",
    "\n",
    "# training the model\n",
    "# iterate over the 20 epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # initialize collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "    # iterate over all mini-batches\n",
    "    for i, (images, labels) in enumerate(fashion_mnist_train_dataloader):\n",
    "        # push mini-batch data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = model(images)\n",
    "        # reset graph gradients\n",
    "        model.zero_grad()\n",
    "        #determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "\n",
    "        # backward pass -- take the loss\n",
    "        loss.backward()\n",
    "        # calculate new parameters based on the loss\n",
    "        optimizer.step()\n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "    \n",
    "    # determine mean mini-batch loss for epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "\n",
    "    # set filename of model -- generate new name for each epoch\n",
    "    model_name = 'fashion_mnist_model_epoch_{}.pth'.format(str(epoch))\n",
    "    # save current model to gdrive models directory\n",
    "    torch.save(model.state_dict(), os.path.join(models_directory, model_name))\n",
    "\n",
    "    # append mean mini-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1NgVSSyKVrI"
   },
   "source": [
    "Based on the loss per epoch, the model is performing better than the base model, at least on the training data. We can also plot this to be able to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFz1pnM_Ikcr"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid(linestyle='dotted') # add grid\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss')\n",
    "\n",
    "# add axis labels and title\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEmqWlgpJJLL"
   },
   "source": [
    "This model looks like it would have converged if given a few more epochs, but it does look to be performing better than the base model so I will not re-run it. I also do not want to risk overfitting to the training data (even though it could be underfitting as it is, but we will see how it performs on the evaluation data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHSgL7JIFUkA"
   },
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "Now we can load the best performing model (the last one) in order to use this on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUQvR9F4JPnP"
   },
   "outputs": [],
   "source": [
    "# restore pre-trained model snapshot\n",
    "best_model_name = '/content/drive/MyDrive/Colab Notebooks/models/fashion_mnist_model_epoch_39.pth'\n",
    "\n",
    "# init pre-trained model class\n",
    "best_model = FashionMnistCNN()\n",
    "\n",
    "# load epoch 19 (best performing epoch) of trained model and put it on cpu (as opposed to gpu)\n",
    "best_model.load_state_dict(torch.load(best_model_name, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv5J6IVRKC1W"
   },
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRkIS5LTN8W2"
   },
   "source": [
    "Now we can evaluate the model and we can use the same fashion_mnist_eval_dataloader defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1wQZJz7KKOH"
   },
   "outputs": [],
   "source": [
    "# initialize collection of mini-batch losses\n",
    "eval_mini_batch_losses = []\n",
    "\n",
    "# iterate over all mini-batches\n",
    "for i, (images, labels) in enumerate(fashion_mnist_eval_dataloader):\n",
    "    \n",
    "    # forward pass\n",
    "    output = best_model(images)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = nll_loss(output, labels)\n",
    "\n",
    "    # collect mini-batch reconstruction loss\n",
    "    eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "# determine mean mini-batch loss for epoch\n",
    "eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "# print epoch loss\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-stRT4nKQf5"
   },
   "source": [
    "Getting the predictions for all observations and the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNBtTamrKP8X"
   },
   "outputs": [],
   "source": [
    "# preds\n",
    "predictions = torch.argmax(best_model(iter(fashion_mnist_eval_dataloader).next()[0]), dim=1)\n",
    "\n",
    "# accuracy\n",
    "metrics.accuracy_score(fashion_mnist_eval_data.targets, predictions.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwXHI-K1PQqh"
   },
   "source": [
    "The accuracy for this model is .9089, which is a huge improvement over the baseline model.\n",
    "\n",
    "Finally, a confusion matrix to show where the highest misclassification happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuqXMUCOPP_i"
   },
   "outputs": [],
   "source": [
    "# define classification matrix of the predicted and target classes\n",
    "mat = confusion_matrix(fashion_mnist_eval_data.targets, predictions.detach())\n",
    "# initialize the plot and define size\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# plot the confusion matrix as a heat map\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=fashion_classes, yticklabels=fashion_classes)\n",
    "plt.tick_params(axis='both', which='major', labelsize=8, labelbottom=False, bottom=False, top=False, left=False, labeltop=True)\n",
    "\n",
    "# plot title and axis labels\n",
    "plt.title('Fashion MNIST Classification Matrix\\nBetter Model\\nAccuracy = 0.9089; Loss=0.2500')\n",
    "plt.xlabel('[true class]')\n",
    "plt.ylabel('[predicted class]')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of exam_exercise.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "gserm_dl_2022env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254.39999389648438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
